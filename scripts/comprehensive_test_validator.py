#!/usr/bin/env python3
"""
Comprehensive Test-AI-Driven Development (TADD) Validator
Ìè¨Í¥ÑÏ†Å ÌÖåÏä§Ìä∏ ÌíàÏßà Í≤ÄÏ¶ù ÎèÑÍµ¨

Ïù¥ Ïä§ÌÅ¨Î¶ΩÌä∏Îäî Îã§ÏùåÏùÑ Í≤ÄÏ¶ùÌï©ÎãàÎã§:
1. ÌÖåÏä§Ìä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ (ÏµúÏÜå 80%)
2. E2E ÌÖåÏä§Ìä∏ Ï°¥Ïû¨ (ÏµúÏÜå 1Í∞ú)
3. Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö© Í≤ÄÏ¶ù
4. ÏÑ±Îä• Î≤§ÏπòÎßàÌÅ¨ Ï∏°Ï†ï
5. AI ÏûëÏÑ± ÌÖåÏä§Ìä∏ ÌíàÏßà
"""

import os
import sys
import subprocess
import json
import glob
from pathlib import Path
from typing import Dict, List, Tuple, Optional

class TADDValidator:
    def __init__(self, project_root: str = "."):
        self.project_root = Path(project_root)
        self.results = {}
        
    def check_coverage(self) -> Dict:
        """ÌÖåÏä§Ìä∏ Ïª§Î≤ÑÎ¶¨ÏßÄ Í≤ÄÏÇ¨"""
        print("üìä Checking test coverage...")
        
        coverage_result = {
            "score": 0,
            "status": "FAIL",
            "details": "No coverage data found"
        }
        
        try:
            # pytest-cov ÏãúÎèÑ
            result = subprocess.run(
                ["python", "-m", "pytest", "--cov=.", "--cov-report=term-missing", "--tb=no"],
                capture_output=True, text=True, cwd=self.project_root
            )
            
            if result.returncode == 0:
                # Ïª§Î≤ÑÎ¶¨ÏßÄ ÌçºÏÑºÌä∏ Ï∂îÏ∂ú
                for line in result.stdout.split('\n'):
                    if 'TOTAL' in line and '%' in line:
                        try:
                            coverage_percent = int(line.split()[-1].replace('%', ''))
                            coverage_result = {
                                "score": coverage_percent,
                                "status": "PASS" if coverage_percent >= 80 else "FAIL",
                                "details": f"Coverage: {coverage_percent}%"
                            }
                        except (ValueError, IndexError):
                            pass
            
        except FileNotFoundError:
            coverage_result["details"] = "pytest not installed"
        
        return coverage_result
    
    def check_e2e_tests(self) -> Dict:
        """E2E ÌÖåÏä§Ìä∏ Ï°¥Ïû¨ ÌôïÏù∏"""
        print("üåê Checking E2E tests...")
        
        # E2E ÌÖåÏä§Ìä∏ ÌååÏùº Ìå®ÌÑ¥
        e2e_patterns = [
            "**/test*e2e*.py",
            "**/e2e*.py", 
            "**/integration*.py",
            "**/*playwright*.py",
            "**/*selenium*.py"
        ]
        
        e2e_files = []
        for pattern in e2e_patterns:
            e2e_files.extend(glob.glob(str(self.project_root / pattern), recursive=True))
        
        return {
            "count": len(e2e_files),
            "status": "PASS" if len(e2e_files) >= 1 else "FAIL",
            "details": f"Found {len(e2e_files)} E2E test files: {[os.path.basename(f) for f in e2e_files[:3]]}"
        }
    
    def check_real_data_usage(self) -> Dict:
        """Ïã§Ï†ú Îç∞Ïù¥ÌÑ∞ ÏÇ¨Ïö© Í≤ÄÏ¶ù"""
        print("üíæ Checking real data usage...")
        
        mock_indicators = ["Mock", "patch", "fake", "stub", "dummy"]
        real_data_indicators = ["fixtures", "database", "api", "file", "csv", "json"]
        
        mock_count = 0
        real_data_count = 0
        
        test_files = glob.glob(str(self.project_root / "**/test*.py"), recursive=True)
        
        for test_file in test_files:
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                for indicator in mock_indicators:
                    mock_count += content.count(indicator)
                    
                for indicator in real_data_indicators:
                    real_data_count += content.count(indicator)
                    
            except Exception:
                continue
        
        real_data_ratio = (real_data_count / (real_data_count + mock_count)) * 100 if (real_data_count + mock_count) > 0 else 0
        
        return {
            "real_data_ratio": round(real_data_ratio, 1),
            "status": "PASS" if real_data_ratio >= 60 else "FAIL",
            "details": f"Real data usage: {real_data_ratio:.1f}% (Mock: {mock_count}, Real: {real_data_count})"
        }
    
    def check_performance_tests(self) -> Dict:
        """ÏÑ±Îä• ÌÖåÏä§Ìä∏ Ï°¥Ïû¨ ÌôïÏù∏"""
        print("‚ö° Checking performance tests...")
        
        perf_patterns = [
            "**/test*perf*.py",
            "**/test*benchmark*.py",
            "**/perf*.py",
            "**/benchmark*.py"
        ]
        
        perf_files = []
        for pattern in perf_patterns:
            perf_files.extend(glob.glob(str(self.project_root / pattern), recursive=True))
        
        # ÏÑ±Îä• Ï∏°Ï†ï ÌÇ§ÏõåÎìú Í≤ÄÏÉâ
        perf_keywords = ["time", "benchmark", "performance", "speed", "latency"]
        perf_test_count = 0
        
        test_files = glob.glob(str(self.project_root / "**/test*.py"), recursive=True)
        for test_file in test_files:
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    content = f.read().lower()
                    if any(keyword in content for keyword in perf_keywords):
                        perf_test_count += 1
            except Exception:
                continue
        
        total_perf_indicators = len(perf_files) + perf_test_count
        
        return {
            "count": total_perf_indicators,
            "status": "PASS" if total_perf_indicators >= 1 else "WARN",
            "details": f"Performance indicators: {total_perf_indicators} (files: {len(perf_files)}, keywords: {perf_test_count})"
        }
    
    def check_ai_test_quality(self) -> Dict:
        """AI ÏûëÏÑ± ÌÖåÏä§Ìä∏ ÌíàÏßà ÌôïÏù∏"""
        print("ü§ñ Checking AI test quality...")
        
        # AI ÏûëÏÑ± ÌÖåÏä§Ìä∏ ÏßÄÌëú
        quality_indicators = {
            "descriptive_names": 0,
            "assertions": 0,
            "edge_cases": 0,
            "documentation": 0
        }
        
        test_files = glob.glob(str(self.project_root / "**/test*.py"), recursive=True)
        
        for test_file in test_files:
            try:
                with open(test_file, 'r', encoding='utf-8') as f:
                    content = f.read()
                    
                # ÏÑ§Î™ÖÏ†Å ÌÖåÏä§Ìä∏ Ïù¥Î¶Ñ
                if "should" in content or "when" in content or "given" in content:
                    quality_indicators["descriptive_names"] += 1
                
                # Ï†ÅÏ†àÌïú assertion Ïàò
                assert_count = content.count("assert")
                if assert_count >= 3:  # ÏµúÏÜå 3Í∞ú assertion
                    quality_indicators["assertions"] += 1
                
                # Ïó£ÏßÄ ÏºÄÏù¥Ïä§ ÌÖåÏä§Ìä∏
                edge_keywords = ["empty", "null", "None", "zero", "negative", "maximum"]
                if any(keyword in content for keyword in edge_keywords):
                    quality_indicators["edge_cases"] += 1
                
                # ÌÖåÏä§Ìä∏ Î¨∏ÏÑúÌôî
                if '"""' in content or "'''" in content:
                    quality_indicators["documentation"] += 1
                    
            except Exception:
                continue
        
        total_quality_score = sum(quality_indicators.values())
        max_possible_score = len(test_files) * 4  # 4 indicators per file
        quality_percentage = (total_quality_score / max_possible_score * 100) if max_possible_score > 0 else 0
        
        return {
            "score": round(quality_percentage, 1),
            "status": "PASS" if quality_percentage >= 70 else "FAIL",
            "details": f"AI test quality: {quality_percentage:.1f}% ({total_quality_score}/{max_possible_score} points)",
            "breakdown": quality_indicators
        }
    
    def run_comprehensive_validation(self) -> Dict:
        """Ï†ÑÏ≤¥ Í≤ÄÏ¶ù Ïã§Ìñâ"""
        print("üéØ Running comprehensive TADD validation...")
        print("=" * 50)
        
        self.results = {
            "coverage": self.check_coverage(),
            "e2e_tests": self.check_e2e_tests(),
            "real_data": self.check_real_data_usage(),
            "performance": self.check_performance_tests(),
            "ai_quality": self.check_ai_test_quality()
        }
        
        # Ï†ÑÏ≤¥ Ï†êÏàò Í≥ÑÏÇ∞
        total_score = 0
        max_score = 0
        
        for category, result in self.results.items():
            if "score" in result:
                total_score += result["score"]
                max_score += 100
            elif result["status"] == "PASS":
                total_score += 100
                max_score += 100
            elif result["status"] == "WARN":
                total_score += 50
                max_score += 100
            else:
                max_score += 100
        
        overall_score = (total_score / max_score * 100) if max_score > 0 else 0
        
        self.results["overall"] = {
            "score": round(overall_score, 1),
            "status": "PASS" if overall_score >= 80 else "FAIL",
            "details": f"Overall TADD compliance: {overall_score:.1f}%"
        }
        
        return self.results
    
    def print_report(self):
        """Í≤ÄÏ¶ù Í≤∞Í≥º Ï∂úÎ†•"""
        print("\n" + "=" * 50)
        print("üìä COMPREHENSIVE TADD VALIDATION REPORT")
        print("=" * 50)
        
        for category, result in self.results.items():
            if category == "overall":
                continue
                
            status_emoji = "‚úÖ" if result["status"] == "PASS" else "‚ö†Ô∏è" if result["status"] == "WARN" else "‚ùå"
            print(f"{status_emoji} {category.upper()}: {result['details']}")
        
        print("\n" + "-" * 50)
        overall = self.results.get("overall", {})
        status_emoji = "‚úÖ" if overall.get("status") == "PASS" else "‚ùå"
        print(f"{status_emoji} OVERALL: {overall.get('details', 'No overall score')}")
        
        if overall.get("status") == "FAIL":
            print("\nüí° Recommendations:")
            if self.results["coverage"]["status"] == "FAIL":
                print("   - Increase test coverage to 80%+")
            if self.results["e2e_tests"]["status"] == "FAIL":
                print("   - Add at least 1 E2E test")
            if self.results["real_data"]["status"] == "FAIL":
                print("   - Use more real data, reduce mocks")
            if self.results["ai_quality"]["status"] == "FAIL":
                print("   - Improve AI test quality (descriptive names, assertions)")


def main():
    if len(sys.argv) > 1:
        project_root = sys.argv[1]
    else:
        project_root = "."
    
    validator = TADDValidator(project_root)
    results = validator.run_comprehensive_validation()
    validator.print_report()
    
    # JSON Ï∂úÎ†• (Îã§Î•∏ Ïä§ÌÅ¨Î¶ΩÌä∏ÏóêÏÑú ÌååÏã± Í∞ÄÎä•)
    with open("tadd_validation_report.json", "w") as f:
        json.dump(results, f, indent=2)
    
    # Ï†ÑÏ≤¥ Í≤ÄÏ¶ù Ïã§Ìå®Ïãú exit code 1
    if results["overall"]["status"] == "FAIL":
        sys.exit(1)
    
    sys.exit(0)


if __name__ == "__main__":
    main()